[{"id":0,"href":"/posts/passchecker-writeup/","title":"Passchecker Writeup","section":"Posts","content":"We were given passchecker2000.py\ndef validate(password: str) -\u0026gt; bool: if len(password) != 49: return False key = ['vs'.join(str(randint(7, 9)) for _ in range(ord(i))) + 'vs' for i in password[::-2]] gate = [118, 140, 231, 176, 205, 480, 308, 872, 702, 820, 1034, 1176, 1339, 1232, 1605, 1792, 782, 810, 1197, 880, 924, 1694, 2185, 2208, 2775] if [randint(a, b[0]) for a, b in enumerate(zip(gate, key), 1) if len(b[1]) != 3 * (b[0] + 7 * a) // a]: return False hammer = {str(a): password[a] + password[a + len(password) // 2] for a in range(1, len(password) // 2, 2)} block = b'c3MxLnRkMy57XzUuaE83LjVfOS5faDExLkxfMTMuR0gxNS5fTDE3LjNfMTkuMzEyMS5pMzIz' if b64encode(b'.'.join([((b + a).encode()) for a, b in hammer.items()])) != block: return False return True if __name__ == \u0026quot;__main__\u0026quot;: passwd = input('Please validate your ID using your password\\n\u0026gt; ') if validate(passwd): print('Access Granted: You now have gained access to the View Source Flag Vault!') else: print('Access Denied :(') the main function takes in a password and determines if it is valid. We want to get a valid password since that would be the flag. The validate function has three conditions that we must pass in order to get the flag.\n1st one is a simple length check:\nreturn False I commented this out so I could examine the workings of the rest of the challenge\nthe second obstacle for the password is a permutation that takes the string provided, goes from back to front, skipping every other letter, and converting to some kind of cipher. key = ['vs'.join(str(randint(7, 9)) for _ in range(ord(i))) + 'vs' for i in password[::-2]] Every other letter from back to front has its ord or ascii number evaluated and used to create a list of string, where each string in the list contains the sequence of [7,8,9]vs, n times where n equals the ord number of the letter being evaluated. A string like a!! would turn into an array of lenght two, the first element containing ord('!') = 33 sequences of [7,8,9]vs \u0026hellip; ie `7vs8vs7vs9vs7vs8vs\u0026hellip;8vs', the second will have ord(\u0026lsquo;a\u0026rsquo;)= 97 sequences of the pattern. The flag information is given in a form of numbers:\n924, 1694, 2185, 2208, 2775] if [randint(a, b[0]) for a, b in enumerate(zip(gate, key), 1) if len(b[1]) != 3 * (b[0] + 7 * a) // a]: return False``` this gooky equation which basically says for each element in the gate, add 7 * current index (starting at 1 not 0) and divide the sum by the current index then convert the product to a character (int to chr) Copying the gate list to a separate python file, we can then compute the flag with this: equation ```a = \u0026quot;\u0026quot; counter = 1 for g in gate: character = chr(int((g + 7*counter)//counter)) counter +=1 a+=character print(a) #v_c_f_T_3_3_F_4_5_w_r___n_i_e_Y_U_t_3_W_0_3_T_M_} This lets us know every other character in the flag. To get the other half of the flag we must go through this challenge:\nblock = b'c3MxLnRkMy57XzUuaE83LjVfOS5faDExLkxfMTMuR0gxNS5fTDE3LjNfMTkuMzEyMS5pMzIz' if b64encode(b'.'.join([((b + a).encode()) for a, b in hammer.items()])) != block: return False this basically takes a block of bytes and encodes indexes and values of the remaining letters. To decode the block we do\nprint(rush) # ['ss1', 'td3', '{_5', 'hO7', '5_9', '_h11', 'L_13', 'GH15', '_L17', '3_19', '3121', 'i323']``` each string has 3 pieces of information encoded like so: `1st character: character 1` `2nd character: character 2` `3rd character onwards: position of the 1st character ` One thing we also seee from the code is that the 2nd character position is always 24 + position of the 1st character1 so we can decrypt this with: ```for a in rush: print(a) index = int(a[2:]) index2 = int(a[2:])+24 value1 = a[0] value2 = a[1] print(index, index2, value1, value2) current_flag[index] = value1 current_flag[index2] = value2 print(current_flag) # vsctf{Th353_FL4G5_w3r3_inside_YOU_th3_WH0L3_T1M3} we don\u0026rsquo;t have to rerun the program with the encrypted version of the flag since we know what the decyrpted version is.\n"},{"id":1,"href":"/posts/lechuck-writeup/","title":"Lechuck Writeup","section":"Posts","content":"Lechuck - HackArmour\nVisiting https://challenges.hackrocks.com/lechuck, checked all storage, cookies, network requests - all clean\nguessed https://challenges.hackrocks.com/lechuck/flag - 404 are handled with a \u0026ldquo;Nothing to see here\u0026hellip;\u0026rdquo; - nothing worthwhile\nRead prompt and tried every word as an endpoint - all 404\nDirbusted with gobuster:\ngobuster -fw -u challenges.hackrocks.com/lechuck/ -w ./common.txt dir | grep \u0026quot;(Status: 200\u0026quot;\ntried 5 different files , opened up two hints and hint #2 said use /user endpoint which yielded this message:\nTo get info about a user, use the syntax: /user/\u0026lt;username\u0026gt;\nusername had to be lechuck from context of the problem which returned flag\n"},{"id":2,"href":"/posts/oblivion-writeup/","title":"Oblivion Writeup","section":"Posts","content":"came with executable (exfiltration)\nhexedit and binwalk showed arm instruction set and a few words but nothing of use\nrunning it produces :\nРаботающий... отправленная информация (1 байт)...[OK] ...(48 times TOTAL) отправленная информация (1 байт)...[OK] АдИос``` which translates to: ```exfiltration tool working.... information sent (1 Byte)..[OK] ...(48 times Total) information sent (1 Byte)..[OK] adios``` I pcap'd networking to see what packets were being sent with `tcpdump -w savetofile.pcap` opened the file in wireshark - important, my computer sends a lot of packets for other applications (spotify, arp etc), I closed those, but the output still wasn't clean, I had to guess that the 48 \u0026quot;information sent\u0026quot; outputs corresponded to 48 DNS queries in wireshark example: `48 6.940687 10.0.0.8 8.8.8.8 DNS 70 Standard query 0x0054 A amazon.com` I tried looking through to see what was different, turns out the \u0026quot;transaction id\u0026quot; was different in each DNS query, in the above example, its 0x0054. I took all 48 DNS queries and looked at the transaction id and wrote them in order here: `47 72 65 65 74 69 6e 67 73 21 20 54 68 65 20 74 6f 6b 65 6e 20 66 6f 72 20 74 68 69 73 20 67 61 6d 65 20 69 73 20 4b 47 46 44 49 44 51 53 51 54` convert this to hex gets you: Greetings! The token for this game is `KGFDIDQSQT` Which was the flag "},{"id":3,"href":"/posts/huge-writeup/","title":"Huge Writeup","section":"Posts","content":"Challenge consists of a challenge file and an output file Output file includes RSA values c (cipher text), n (modulus), and e (exponent) In order to decrypt a file encrypted with an RSA encryption, according to wikipedia, one must solve the following equation:\n(c^d)modn\nn and c are given but we don\u0026rsquo;t know d\nd can be found by solving this equation:\nde=1mod(lambda(n))\ne is known, lambda(n) is not.\nwe can find this value by using an online euler\u0026rsquo;s totient calculator.\nI used this one : https://comnuan.com/cmnn02/cmnn02005/\nI didn\u0026rsquo;t have the time to figure out how to calculate this without these calculators.\nAfter we have this number, we can plug it into the equation and solve. I coded a fast solution below.\nfrom Crypto.Util.number import long_to_bytes n = 257827703087398016057355158654193468564980243813004452658087616586210487667215030370871398983230710387803731676134007721137156696714627083072326445637415561591372586919746606752675050732692230618293581354674196658443898625965651230501721590806987488038754683843111434873697465691139129703835890867256688046172118591 c = 194667317703687479298989188290833629421904543231503224641743768867721632949682167895699280370759100055314992068135383846690184090232092994595979623784341194946153538127175310278245722299688212621004144260665233469561373125699948009903943019071999887294005117156960295183926108287198987970959145603429337056005819069 e = 65537 lambda_n = 238940154401626938037848370480225183045581769211725031481021020007970362783588965693807273855047475931562553093129263532876569906106451113480591159727935347588408235764799039034556484596551260176507450085252133677350349066373844187940165049949179091946213517645837708200090091217192271487206162432000000000000000000 # since de=1mod(lambda(n)) we know that de % lambda(n) has to equal 1. # this means d and e have to multiple to be 1 greater than a multiple of lambda_n # get a sheet of paper and try to calculate d with small e and lambda(n) primes. factor = (lambda_n//e) + 1 x = lambda_n # iterative method to find d while True: if (x+1)%e == 0: d= (x+1)//e break x += lambda_n # this solves the first stated decryption equation and converts the result to string # first by converting the integer to bytes then to string with decode() print(long_to_bytes(pow(c,d,n)).decode()) output: ictf{sm4ll_pr1mes_are_n0_n0_9b129443}  "},{"id":4,"href":"/posts/mlevsma/","title":" Maximum Log Likelihood (MLE) vs Maximum Aposterior (MAP)","section":"Posts","content":"The following\n$$\\int_{a}^{b} x^2 dx$$\nIs an integral\nMaximum Log Likelihood (MLE) vs Maximum Aposterior (MAP) #  If you\u0026rsquo;re like me, this whole maximum log likihood vs bayesian ordeal mumbo jumbo has made you dizzy. The first time I learned this, I didn\u0026rsquo;t quite understand what the fuss was about. Why are we using \u0026ldquo;Log\u0026rdquo; and \u0026ldquo;Likelihood\u0026rdquo; and why does \u0026ldquo;Aposterior\u0026rdquo; sound so foreign. I\u0026rsquo;ve made a short blog post to help those out that, like me, didn\u0026rsquo;t quite get it the first time.\nWhat is a Maximum Likelihood? #  The fundamental idea you have to understand about Maximum Likelihood is that it tries to ask the question, what is the best \u0026ldquo;model\u0026rdquo; or \u0026ldquo;estimate\u0026rdquo; that we can give justby looking at the data we have collected. It\u0026rsquo;s actually really simple when we look at an example.\nCoin Flip Example #  We want to get the probability of this coin landing on heads. We flip it 10 times to determine this coin\u0026rsquo;s probability of landing heads. Let\u0026rsquo;s say we get 6 heads and 4 tails. Maximum Likelihood says the probability of getting heads is $$P(X=Heads) = 6/10=.6$$. Based on our data, and no prior belief of the coin\u0026rsquo;s chances, Maximum Likelihood tries to predict the probability. We will go over the math in a bit.\nWhat about Maximum Aposterior? #  Maximum Aposterior (MAP) takes a bayesian approach to this (aka using prior knowledge to help get a more accurate model) which can be particularly useful if you don\u0026rsquo;t have too much data to let the Law of Large Numbers do its job. Now when you flip a fair coin, you should expect to see that you get about 5 heads in 10 flips. However, in our experiment, we got 6 heads in 10 flips. Based on our experience, coins typically give a 50/50 fair chance, so we can incorporate this into our model to improve its accuracy with low data. With a Maximum Aposterior estimate, we use both our data and our belief to make the model. Since our \u0026ldquo;Prior\u0026rdquo; is $$P(X=Heads)=.5$$ and our actual experiment yielded $$P(X=Heads)=.6$$, our Maximum Aposterior should yield somewhere between $$.5$$ and $$.6$$ probabiliy. Now I know this is vague, but with baysian probability the exact probability estimate will be determined by how much we believe in our prior vs our data. If we think we are super sure that a coin has .5 probability because its a really special coin made only for NFL coin flips, then we may weigh our prior to be harder to change, meaning our MAP estimator will output something very close to $$.5$$ unless there is overwhelming evidence pulling it elsewhere.\nMath time #  Maximum Likelihood Estimator #  Now from the coin flip example, it seems like the way you determine the maximum liklihood is just by getting the simple probability. This just so happens to be the case when we are getting the maximum liklihood of a Univariate Gaussian Distribution. The maximum liklihood of a some data tries to estimate the true parameters of the population that the data came from. In our instance, we assume outcomes of the coin flip correspond to sum normaldistribution. With this assumption, we will work out why the MLE of a coin flip data is just the mean of our data. With our coin example, we will first make it more formal:\nUsing this, we can write the data as $$X=(x_1,\\dots,x_n)$$ where:\n$$ x_i = \\left{ \\begin{array}{ll} 1 \u0026amp; \\quad \\text{if Heads} \\ 0 \u0026amp; \\quad \\text{if Tails} \\end{array} \\right. $$\nNow let\u0026rsquo;s define the MLE:\n$$\\theta_{MLE} = \\max_{\\theta \\epsilon \\Theta}P(X| \\theta)$$\nOkay so $$P(X ⎮ \\theta)$$, what does it mean:\n$$P(X|\\theta) =P(x_1,\\dots,x_n | \\theta) = \\prod_{i=1}^nP(x_i = x_i | \\theta)$$\nPutting it together, this is saying we need to pick parameters $$\\theta$$ that maximizes the probability that each data point came from the probability distribution we are estimating the data points to come from. In our coin flip example, this means we need to pick a value that we can say, for a Bernoulli case of 10 coin flips where 6 outcomes are heads, what would be the average value we expect the next coin flip to be based on the previous 10 flips. This value works out to .6, and here\u0026rsquo;s how we determine that:\nAssume the coin flips $$X$$ comes from a normal distribution, $$X \\sim N(\\mu, \\sigma^2)$$, so that means\n$$P(x|\\theta) = \\frac{1}{\\sqrt(2\\pi\\sigma^2)}\\exp(-\\frac{1}{2\\sigma^2}(x-\\theta)^2)$$\nNote that $$\\theta$$ in this example means the $$\\mu$$ that we are estimating, but we will continue to use $$\\theta$$ for formality.\nSo essentially we want to maximize this probability term for every X by choosing a $$\\theta$$ that maximizes it. But in math, minimizing a product of probabilities is hard, because taking the derivative of a huge product is quite ugly. We will use a trick by instead taking the log of the function. Using a log, we can split up the function and use the Log Product Rule to make this problem look a little easier:\n$$\\log(P(X|\\theta)) = -\\frac{n}{2}\\log(2\\pi\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(x_i-\\theta)^2$$\nPlease refer to the Log Rules for how we got to this. Its also important to note that we can use log because log(x) is a monotonically increasing function and maximizing $$P(X)$$ is the same as maximizing $$\\log(P(X))$$. Now once we have this, we are ready to differentiate and set equal to 0 in order to find our critical points:\n$$0 = \\frac{d}{d\\theta}\\log(P(X|\\theta) = \\frac{1}{2\\sigma^2}\\sum2(x_i-\\theta) = \\frac{1}{\\sigma^2}(\\sum{}x_i-n\\theta) = 0$$\n$$\\Rightarrow\\frac{1}{n}\\sum_{i=1}^nx_i = \\theta_{MLE} = \\bar{x}$$\nThat\u0026rsquo;s crazy how this all simplifies down to $$\\theta_{MLE} = \\bar{x}$$, which means maximizing our likelihood is just choosing the sample mean. BLows your MIND right?\nOK. Maybe not, but if that doesn\u0026rsquo;t the MAP math will.\nAside: Now sometimes people will call the likelihood as the log likelihood, thats just because we maximize that instead, but its really doing the same thing, so most people use the likelihood and log likelihood synonymously.\nMaximium Aposterior #  Now up until this point, we were pretty vague about what a prior meant, but let\u0026rsquo;s make it absolutely clear what we mean by a prior. The prior refers to the $$P(A)$$ term in Bayes' Theorem.\n$$P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$$\nThe top term is basically the maximim likelihood times a prior, this is the fundamental difference between the two, and you will see why this makes a difference as we work through the equations. So let\u0026rsquo;s first define the the Maximum Aposterior.\n$$\\theta_{MAP} = \\max_{\\theta \\epsilon \\Theta}P(\\theta | X)$$\nCompare this function to the MLE objective function. What looks different?\nIf you noticed, we are maximizing $$P(\\theta⎮X)$$ rather than $$P(X⎮\\theta)$$. This essentially is saying we want to maxmize the posterior distribution in $\\theta$. This fancy mumbo jumbo can be broken down to someting understandable. \u0026ldquo;Post\u0026rdquo; means after and \u0026ldquo;erior\u0026rdquo; refers to the the prior, so we want to maximize the probability of our after-prior distribution, or distribution we have after a prior knowledge of thinking its a fair coin. So what this objective function really is saying is:\n$$\\theta_{MAP} = \\max_{\\theta \\epsilon \\Theta}P(X | \\theta)P(\\theta)$$\nwhich if you notice, comes from the numerator of Bayes rule. We can throw out the denominator of Bayes rule because maximizing the top is also maximizing the whole fraction.\nOkay, let\u0026rsquo;s get back to the math. In this problem, we also use the log to maximize so we can deal with each of those P terms separately using the log product rule. This will make our new objective function look like:\n$$\\theta_{MAP} = \\max_{\\theta \\epsilon \\Theta}\\log(P(X | \\theta)) + \\log(P(\\theta))$$\nLet\u0026rsquo;s just differentiate and set these bad boys equal to zero to maximize:\n$$0 = \\frac{d}{d\\theta}(\\log(P(X|\\theta)) + \\log(P(\\theta)))$$\nWe already have the derivate of the first term since we already did it in MLE. It comes out to\n$$\\frac{d}{d\\theta}\\log(P(X|\\theta)) = \\frac{1}{\\sigma^2}(\\sum{}x_i-n\\theta)$$\nNow we just have to take the derivative of the second term.\n$$\\frac{d}{d\\theta}\\log(P(\\theta))$$\nIn order to differentiate this we need to define it:\n$$P(\\theta) = \\frac{1}{\\sqrt(2\\pi\\sigma^2)}\\exp(-\\frac{1}{2\\sigma^2}(x-\\mu)^2)$$\nAnd now we take the log and differentiate this:\n$$\\frac{d}{d\\theta}\\log(P(\\theta)) =\\frac{d}{d\\theta}(-\\frac{1}{2}\\log(2\\pi) - \\frac{1}{2}(\\theta-\\mu)^2)$$\n$$= \\theta-\\mu$$\nSo now putting these together, we have\n$$\\frac{d}{d\\theta}(\\log(P(X|\\theta)) + \\log(P(\\theta)))= 0$$\n$$\\Rightarrow 0 = \\frac{1}{\\sigma^2}(\\sum{}x_i-n\\theta) + \\mu + \\theta$$\nWe rearrange this to get:\n$$\\Rightarrow \\sum_{i=1}^n\\frac{x_i}{\\sigma^2} + \\mu = \\theta(\\frac{n}{\\sigma^2} + 1)$$\n$$\\Rightarrow\\theta_{MAP} = \\frac{n}{n+\\sigma^2}\\bar{x} + \\frac{\\sigma^2}{n+\\sigma^2}\\mu$$\nI skipped some algebra, so go back and work out the last few steps yourself. But I wanted to get to the intuition behind this result.\nAs you can see, the left side of this equation is the sample mean, and the right side is the prior mean. When the n is small, or you only flip the coin a few times, the term on the right will dominate more. However, as $$n\\rightarrow\\infty$$, the value of $$\\theta_{MAP}$$ is mainly composed of the left term, as the right term goes to 0. Meaning, when we flip the coin a lot, and it keeps landing 6/10 times heads, we stop believing it was a fair sided NFL coin and more like a rigged unfair coin. So as $$n\\rightarrow\\infty$$, MAP looks a lot like $$\\bar{x}$$, which is the MLE.\nConclusion #  Whether you use MLE or MAP is up to you, if you think you the data is what you should base your estimates on and nothing else, MLE is the way to go. If you think you know something that will help the problem, especially if there isn\u0026rsquo;t enough data or the data isn\u0026rsquo;t that good, you might want to use a Bayesian model like MAP.\n"}]